




<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>Deploying LLM web applications on Open OnDemand: experiences at Perth Children&#x27;s Hospital ::  GOOD 2025 :: pretalx</title>
        <meta name="title" content="Deploying LLM web applications on Open OnDemand: experiences at Perth Children&#x27;s Hospital  - GOOD 2025 pretalx">
        <meta name="description" content="We are a team of multidisciplinary paediatric anaesthesia researchers at Perth Children’s Hospital in Western Australia. Real-world deployments of HPC in healthcare settings have been limited by the challenges of working with sensitive healthcare data within restricted network and operational environments. In Australia, health data cannot be processed outside of the country and in most cases cannot leave the hospital data centre due to privacy requirements. The large Australian public supercomputers NCI GADI and Pawsey Supercomputing Centre do not permit the storage and analysis of healthcare data. Accordingly, Australian medical HPC research has focused on genomic analysis, drug discovery and corporate AI model development. Recently, we deployed a small on-premises HPC research cluster with Open OnDemand (OOD) inside Perth Children’s Hospital primarily to facilitate the use of large language models (LLMs) on sensitive hospital data. 
Our team comprises software engineers, data analysts and end users i.e. research assistants, anaesthetists and other healthcare staff who use AI models as part of everyday clinical work. We deployed OOD with the intention of enabling a variety of interactive and batch computing jobs to suit the needs of our users. We realised that there are many containerised low-code LLM web applications that would be suitable to use by non-technical users and that can be contextualised with locally developed language models and techniques such as retrieval augmented generation (RAG). 
We implemented a containerised workflow using Podman Compose to orchestrate container services and to expose web services over the OOD reverse proxy. Our environment constraints mean that most users share the same node, so we additionally restrict applications to only expose ports over an authenticated Caddy reverse proxy. The hospital environment is strictly locked down and as such some web applications required patching to render client-side URLs prefixed with the OOD reverse proxy path. Future work to simplify this process is planned.
This enables an end-to-end workflow where technical researchers are able to use batch jobs to finetune and deploy models locally for use by non-technical end users in clinical situations using open-source user-friendly web applications over OOD. We believe this workflow democratises access to LLM and HPC resources and allows our team to be more agile in developing and validating LLMs in clinical settings.">
        <meta name="application-name" content="pretalx">
        <meta name="generator" content="pretalx">
        <meta name="keywords" content="GOOD 2025, 2025, 2025, schedule, talks, cfp, call for papers, conference, submissions, organizer">
        
        <meta name="robots" content="index, follow">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#C91235">
        <meta name="HandheldFriendly" content="True"/>
        <meta property="thumbnail" content="https://cfp.openondemand.org/2025/talk/ZHJDGC/og-image">
    <meta property="og:image" content="https://cfp.openondemand.org/2025/talk/ZHJDGC/og-image">
        <meta property="og:title" content="Deploying LLM web applications on Open OnDemand: experiences at Perth Children&#x27;s Hospital GOOD 2025">
        <meta property="og:description" content="We are a team of multidisciplinary paediatric anaesthesia researchers at Perth Children’s Hospital in Western Australia. Real-world deployments of HPC in healthcare settings have been limited by the challenges of working with sensitive healthcare data within restricted network and operational environments. In Australia, health data cannot be processed outside of the country and in most cases cannot leave the hospital data centre due to privacy requirements. The large Australian public supercomputers NCI GADI and Pawsey Supercomputing Centre do not permit the storage and analysis of healthcare data. Accordingly, Australian medical HPC research has focused on genomic analysis, drug discovery and corporate AI model development. Recently, we deployed a small on-premises HPC research cluster with Open OnDemand (OOD) inside Perth Children’s Hospital primarily to facilitate the use of large language models (LLMs) on sensitive hospital data. 
Our team comprises software engineers, data analysts and end users i.e. research assistants, anaesthetists and other healthcare staff who use AI models as part of everyday clinical work. We deployed OOD with the intention of enabling a variety of interactive and batch computing jobs to suit the needs of our users. We realised that there are many containerised low-code LLM web applications that would be suitable to use by non-technical users and that can be contextualised with locally developed language models and techniques such as retrieval augmented generation (RAG). 
We implemented a containerised workflow using Podman Compose to orchestrate container services and to expose web services over the OOD reverse proxy. Our environment constraints mean that most users share the same node, so we additionally restrict applications to only expose ports over an authenticated Caddy reverse proxy. The hospital environment is strictly locked down and as such some web applications required patching to render client-side URLs prefixed with the OOD reverse proxy path. Future work to simplify this process is planned.
This enables an end-to-end workflow where technical researchers are able to use batch jobs to finetune and deploy models locally for use by non-technical end users in clinical situations using open-source user-friendly web applications over OOD. We believe this workflow democratises access to LLM and HPC resources and allows our team to be more agile in developing and validating LLMs in clinical settings.">
        <meta property="og:url" content="http://testserver/2025/talk/ZHJDGC/">
        <meta property="twitter:card" content="summary">

        
            <link rel="icon" type="image/png" sizes="32x32" href="/good2025/schedule/static/common/img/favicon-32x32.38c2aab28d3a.png">
            <link rel="icon" type="image/png" sizes="16x16" href="/good2025/schedule/static/common/img/favicon-16x16.02f8a56e1af3.png">
        

        <link rel="stylesheet" type="text/css" href="/good2025/schedule/static/common/scss/uncompressed.cabbf3759e3e.css" />
        
            <link rel="stylesheet" type="text/css" href="/good2025/schedule/static/vendored/fullcalendar/fullcalendar.min.ba055a2cd2b3.css"/>
            <link rel="stylesheet" type="text/css" href="/good2025/schedule/media/2025/cfp.b4fdc45e320a46e0.css"/>
            <link rel="stylesheet" type="text/css" href="/good2025/schedule/media/2025/agenda.4ecfa7153a486dbb.css"/>
        
        
        <script src="/good2025/schedule/static/CACHE/js/output.079684506270.js" defer></script>
        
    
    <link rel="alternate" type="application/json" title="GOOD 2025 API" href="https://cfp.openondemand.org/api/events/2025/talks/ZHJDGC" />

    
    

    </head>
    <body data-datetimeformat="YYYY-MM-DD HH:mm:ss" data-dateformat="YYYY-MM-DD" data-datetimelocale="en">
        <div id="top-bg" class="header pcb">
            
        </div>
        
        <div class="container" id="main-container">
            <header>
                <h1>
                    <a href="
    /2025/schedule/
">
                        
                            GOOD 2025
                        
                    </a>
                </h1>

                <div class="d-flex" id="header-row">
                    
                    
                </div>
            </header>
            <div class="card" id="main-card">
                <main>
                    

                    
    
    
    <article>
        
        <h3 class="talk-title">
            <div class="heading-with-buttons">
                Deploying LLM web applications on Open OnDemand: experiences at Perth Children&#x27;s Hospital
                <div class="buttons" id="talk-buttons">
                    
                        <a class="btn btn-outline-primary" href="/good2025/schedule/talk/ZHJDGC.ics">
                            <i class="fa fa-calendar"></i> .ical
                        </a>
                    
                    
                    
                </div>
            </div>
            <small>
                
                    03-18, 16:00–16:25 (US/Eastern), Belfer Case Study Room (CGIS S020) <i class="fa fa-question-circle" data-toggle="tooltip" title="S020"></i>
                
                
                
            </small>
        </h3>
        <div class="talk row">
            <div class="talk-content">
                
                
                
                <section class="abstract">
                    <p>Open OnDemand (OOD) is ideally placed to bridge the divide between traditional shell-based batch HPC and emerging large language model (LLM) workflows. Our team deployed OOD internally at MERLIN, a small HPC research cluster located inside Perth Children’s Hospital to provide HPC resources to a range of technical and non-technical users on sensitive healthcare data. We adapted existing LLM web applications for OOD to provide a low-code playground for non-technical researchers such as medical doctors and research assistants to engage with LLM resources in a healthcare environment. This presentation will discuss the rationale, and implementation of OOD LLM web applications in a highly restricted environment and motivate future improved support for these types of workflows within OOD.</p>
                </section>
                
                    <hr />
                
                <section class="description">
                    <p>We are a team of multidisciplinary paediatric anaesthesia researchers at Perth Children’s Hospital in Western Australia. Real-world deployments of HPC in healthcare settings have been limited by the challenges of working with sensitive healthcare data within restricted network and operational environments. In Australia, health data cannot be processed outside of the country and in most cases cannot leave the hospital data centre due to privacy requirements. The large Australian public supercomputers NCI GADI and Pawsey Supercomputing Centre do not permit the storage and analysis of healthcare data. Accordingly, Australian medical HPC research has focused on genomic analysis, drug discovery and corporate AI model development. Recently, we deployed a small on-premises HPC research cluster with Open OnDemand (OOD) inside Perth Children’s Hospital primarily to facilitate the use of large language models (LLMs) on sensitive hospital data. <br>
Our team comprises software engineers, data analysts and end users i.e. research assistants, anaesthetists and other healthcare staff who use AI models as part of everyday clinical work. We deployed OOD with the intention of enabling a variety of interactive and batch computing jobs to suit the needs of our users. We realised that there are many containerised low-code LLM web applications that would be suitable to use by non-technical users and that can be contextualised with locally developed language models and techniques such as retrieval augmented generation (RAG). <br>
We implemented a containerised workflow using Podman Compose to orchestrate container services and to expose web services over the OOD reverse proxy. Our environment constraints mean that most users share the same node, so we additionally restrict applications to only expose ports over an authenticated Caddy reverse proxy. The hospital environment is strictly locked down and as such some web applications required patching to render client-side URLs prefixed with the OOD reverse proxy path. Future work to simplify this process is planned.<br>
This enables an end-to-end workflow where technical researchers are able to use batch jobs to finetune and deploy models locally for use by non-technical end users in clinical situations using open-source user-friendly web applications over OOD. We believe this workflow democratises access to LLM and HPC resources and allows our team to be more agile in developing and validating LLMs in clinical settings.</p>
                </section>
                
            </section>
            
            <section>
                
            </section>
        </div>
    </div>
    
        
            <div class="pretalx-session">
                <div class="pretalx-session-time-box avatar">
                    <a href="/good2025/schedule/speaker/MVBZY3/">
                        <div class="avatar-wrapper">
                            
                                <img src="/good2025/schedule/speaker/avatar.svg">
                            
                        </div>
                    </a>
                </div>
                <div class="pretalx-session-info">
                    <div class="title">
                        <a href="/good2025/schedule/speaker/MVBZY3/">Harry Smallbone [University of Western Australia]</a>
                    </div>
                    <div class="abstract"><p>Harry is a junior medical doctor in Western Australia working in internal medicine and a PhD candidate at the University of Western Australia investigating the use of artificial intelligence in paediatric anaesthesia. His Honours thesis was in using Computer Vision for myeloproliferative neoplasms. He has previously been a student at Pawsey Supercomputing Centre and is a sysadmin of MERLIN, a small GPU research cluster located within Perth Children's Hospital for research on sensitive healthcare data. His research interests encompass large language models, digital health and computer vision.</p></div>
                    
                </div>
            </div>
        
            <div class="pretalx-session">
                <div class="pretalx-session-time-box avatar">
                    <a href="/good2025/schedule/speaker/UU9PCJ/">
                        <div class="avatar-wrapper">
                            
                                <img src="/good2025/schedule/speaker/avatar.svg">
                            
                        </div>
                    </a>
                </div>
                <div class="pretalx-session-info">
                    <div class="title">
                        <a href="/good2025/schedule/speaker/UU9PCJ/">Thomas Drake-Brockman [Perth Children&#x27;s Hospital]</a>
                    </div>
                    <div class="abstract"><p>Dr Thomas Drake-Brockman (they/them) is a medical doctor and clinician-technologist working across Perth Children's Hospital, the University of Western Australia, and The Kids Research Institute Australia.</p></div>
                    
                </div>
            </div>
        
    
    </article>


                </main>
            </div>
            <footer>
                
                    <div id="exporttimestamp" class="text-muted">
                        
                        This is a static export generated at 2025-07-09 17:45 EDT
                    </div>
                
                


    powered by <a href="https://pretalx.com" target="_blank" rel="noopener">pretalx</a>


                
                    &middot;
                    <a href="mailto:jim@numfocus.org">Contact us</a>
                
                
                
            </footer>
        </div>
    </body>
</html>
